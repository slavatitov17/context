# Настройка LLM для генерации ответов

Система использует локальные LLM модели через `@xenova/transformers` для генерации качественных ответов.

## Используемая модель

### Mistral 7B Instruct (Mistral AI) - Основная модель
- **Модель**: `Xenova/Mistral-7B-Instruct-v0.2`
- **Размер**: ~7GB (квантованная версия)
- **Производитель**: Mistral AI (французская компания)
- **Языки**: Многоязычная, отличная поддержка русского
- **Преимущества**: 
  - Высокое качество ответов
  - Быстрая работа в России (не Google)
  - Хорошо понимает инструкции
  - Оптимизирована для диалогов

## Как это работает

1. **Автоматическая загрузка**: При первом запросе система автоматически загрузит модель Mistral 7B
2. **Кэширование**: Модель кэшируется в памяти после первой загрузки
3. **Fallback**: Если модель не загрузилась, используется простая генерация на основе контекста

## Настройка модели

Модель настроена в файле `app/api/rag/query/route.ts`:

```typescript
// Используется Mistral 7B Instruct
textGenerationModel = await pipeline(
  'text-generation',
  'Xenova/Mistral-7B-Instruct-v0.2',
  { quantized: true }
);
```

Модель загружается автоматически при первом запросе и кэшируется в памяти.

## Требования

- **Память**: Минимум 8GB RAM (рекомендуется 16GB+)
- **Диск**: ~7GB свободного места для хранения модели
- **Интернет**: Требуется для первой загрузки модели (после этого работает офлайн)

## Производительность

- **Первая загрузка**: 2-5 минут (в зависимости от скорости интернета)
- **Генерация ответа**: 3-10 секунд (в зависимости от длины контекста)
- **Использование памяти**: ~8GB RAM (квантованная версия)

## Отключение LLM

Если вы хотите использовать только простую генерацию ответов (без LLM), просто не загружайте модель. Система автоматически переключится на простую генерацию на основе контекста.

## Безопасность

- Все модели загружаются локально
- Данные не передаются третьим сторонам
- Соответствует требованиям 152-ФЗ
- Работает полностью офлайн после первой загрузки
