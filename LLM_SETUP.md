# Настройка LLM для генерации ответов

Система поддерживает использование локальных LLM моделей через `@xenova/transformers` для генерации более качественных ответов.

## Рекомендуемые модели (совместимы с Россией)

### 1. Gemma 2B (Google) - Рекомендуется
- **Модель**: `Xenova/gemma-2-2b-it`
- **Размер**: ~2GB
- **Производитель**: Google (не Meta)
- **Языки**: Многоязычная, включая русский
- **Преимущества**: Легковесная, быстрая, хорошее качество

### 2. Mistral 7B (Mistral AI)
- **Модель**: `Xenova/Mistral-7B-Instruct-v0.2`
- **Размер**: ~7GB
- **Производитель**: Mistral AI (французская компания)
- **Языки**: Многоязычная
- **Преимущества**: Высокое качество, но требует больше памяти

### 3. Qwen 2.5 (Alibaba)
- **Модель**: `Xenova/Qwen2.5-1.5B-Instruct`
- **Размер**: ~1.5GB
- **Производитель**: Alibaba (китайская компания)
- **Языки**: Многоязычная, отличная поддержка русского
- **Преимущества**: Очень легковесная, быстрая

## Как это работает

1. **Автоматическая загрузка**: При первом запросе система автоматически загрузит выбранную модель
2. **Кэширование**: Модель кэшируется в памяти после первой загрузки
3. **Fallback**: Если модель не загрузилась, используется простая генерация на основе контекста

## Настройка модели

Модель настраивается в файле `app/api/rag/query/route.ts`:

```typescript
// Текущая модель: Gemma 2B
textGenerationModel = await pipeline(
  'text-generation',
  'Xenova/gemma-2-2b-it',
  { quantized: true }
);
```

### Изменение модели

Чтобы использовать другую модель, измените строку в функции `getTextGenerationModel()`:

```typescript
// Для Qwen 2.5
textGenerationModel = await pipeline(
  'text-generation',
  'Xenova/Qwen2.5-1.5B-Instruct',
  { quantized: true }
);

// Для Mistral 7B (требует больше памяти)
textGenerationModel = await pipeline(
  'text-generation',
  'Xenova/Mistral-7B-Instruct-v0.2',
  { quantized: true }
);
```

## Требования

- **Память**: Минимум 4GB RAM для Gemma 2B, 8GB+ для Mistral 7B
- **Диск**: ~2-7GB свободного места для хранения моделей
- **Интернет**: Требуется для первой загрузки модели (после этого работает офлайн)

## Производительность

- **Первая загрузка**: 1-5 минут (в зависимости от модели и скорости интернета)
- **Генерация ответа**: 2-10 секунд (в зависимости от модели и длины контекста)
- **Использование памяти**: 2-8GB RAM в зависимости от модели

## Отключение LLM

Если вы хотите использовать только простую генерацию ответов (без LLM), просто не загружайте модель. Система автоматически переключится на простую генерацию на основе контекста.

## Безопасность

- Все модели загружаются локально
- Данные не передаются третьим сторонам
- Соответствует требованиям 152-ФЗ
- Работает полностью офлайн после первой загрузки


