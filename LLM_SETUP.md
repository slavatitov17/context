# Настройка LLM для генерации ответов

Система использует локальные LLM модели через `@xenova/transformers` для генерации качественных ответов.

## Используемая модель

### Mistral 7B Instruct (Mistral AI) - Основная модель
- **Модель**: `Xenova/Mistral-7B-Instruct-v0.2`
- **Размер**: ~7GB (квантованная версия)
- **Производитель**: Mistral AI (французская компания)
- **Языки**: Многоязычная, отличная поддержка русского
- **Преимущества**: 
  - Высокое качество ответов
  - Быстрая работа в России (не Google)
  - Хорошо понимает инструкции
  - Оптимизирована для диалогов

## Как это работает

1. **Автоматическая загрузка**: При первом запросе система автоматически загрузит выбранную модель
2. **Кэширование**: Модель кэшируется в памяти после первой загрузки
3. **Fallback**: Если модель не загрузилась, используется простая генерация на основе контекста

## Настройка модели

Модель настраивается в файле `app/api/rag/query/route.ts`:

```typescript
// Текущая модель: Gemma 2B
textGenerationModel = await pipeline(
  'text-generation',
  'Xenova/gemma-2-2b-it',
  { quantized: true }
);
```

### Изменение модели

Чтобы использовать другую модель, измените строку в функции `getTextGenerationModel()`:

```typescript
// Для Qwen 2.5
textGenerationModel = await pipeline(
  'text-generation',
  'Xenova/Qwen2.5-1.5B-Instruct',
  { quantized: true }
);

// Для Mistral 7B (требует больше памяти)
textGenerationModel = await pipeline(
  'text-generation',
  'Xenova/Mistral-7B-Instruct-v0.2',
  { quantized: true }
);
```

## Требования

- **Память**: Минимум 8GB RAM (рекомендуется 16GB+)
- **Диск**: ~7GB свободного места для хранения модели
- **Интернет**: Требуется для первой загрузки модели (после этого работает офлайн)

## Производительность

- **Первая загрузка**: 2-5 минут (в зависимости от скорости интернета)
- **Генерация ответа**: 3-10 секунд (в зависимости от длины контекста)
- **Использование памяти**: ~8GB RAM (квантованная версия)

## Отключение LLM

Если вы хотите использовать только простую генерацию ответов (без LLM), просто не загружайте модель. Система автоматически переключится на простую генерацию на основе контекста.

## Безопасность

- Все модели загружаются локально
- Данные не передаются третьим сторонам
- Соответствует требованиям 152-ФЗ
- Работает полностью офлайн после первой загрузки


